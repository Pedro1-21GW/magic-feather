# ü™∂ The Magic Feather: DnD Speech-to-Text Transcription Pipeline ü™∂
This project provides a Python script (`DnD_Transcription.py`) designed to transcribe long, multi-speaker audio sessions (like Dungeons & Dragons) into a single, chronologically-sorted "movie script" file. Just to give you a practical scenario: using Google's Colab free tier T4 GPU this ran on about 3 hours for 4 hour long (each) audio files.
It's built to work with multi-track recordings, such as those generated by the [Discord Craig bot](https://craig.chat/), where each speaker has their own separate audio file.
It uses only open source models, for the ASR transcription and also for VAD.

## ü™∂ Core Features:
* **High-Quality Transcription**: Utilizes **OpenAI's Whisper** `medium` model for high quality (WER is considerably low, even in non-English languages (like Brazilian Portuguese)).
* **Intelligent VAD Pre-processing**: Employs **Silero-VAD** to scan files and find speech chunks *before* transcription. This saves massive amounts of processing time by skipping hours of silence.
* **True Speaker Diarization**: By using separate files per player, it achieves perfect "who said what" identification (diarization). The player's filename is used as their speaker name.
* **Resilient Checkpointing**: Saves a separate `_log.txt` for each player as it finishes processing them. If the script is stopped and re-run, it will skip any players who already have a log file, picking up right where it left off.
* **Large File Handling**: Includes a built-in mechanism to split a single, exceptionally large audio file (e.g., a very talkative DM) into multiple parts, preventing crashes and making the process more manageable.
* **Final Chronological Merge**: After all individual logs are created, the script reads *all* of them, sorts every single line from every file by its timestamp, and writes one beautiful, final transcript.

## ‚öôÔ∏è How It Works:
1.  **Load Models**: The script first loads the Whisper model and the Silero-VAD model into memory (using your GPU if available).
2.  **Scan Files**: It scans the `AUDIO_FOLDER` for all `.flac` files.
3.  **Check for Logs**: For each audio file (e.g., `Player1.flac`), it checks if a corresponding log file (e.g., `Player1_log.txt`) already exists in the `OUTPUT_LOG_FOLDER`. If it does, it skips that player and moves to the next one.
4.  **Apply VAD**: If no log is found, it loads the audio file, converts it to the required format (16kHz, 16-bit, mono), and uses Silero-VAD to get a list of all timestamps where speech occurs.
5.  **Chunk & Transcribe**: It iterates through the VAD speech chunks. For each chunk:
    * It slices the *original* audio file using `pydub` (`audio[start_ms:end_ms]`).
    * It exports this tiny slice to a `temp_chunk.wav` file.
    * It runs Whisper transcription on *only* that chunk.
6.  **Fix Timestamps**: This is the magic. Whisper returns timestamps relative to the chunk (e.g., `0.5s`). The script calculates the **global session timestamp** by adding the chunk's start time (`start_ms`) to Whisper's segment start time.
7.  **Save Individual Log**: After all chunks for a player are transcribed, it saves their personal `Player1_log.txt`, with all lines correctly timestamped and formatted.
8.  **Final Merge**: Once all players are processed, the script takes all the transcript entries it has gathered (from new processing and loaded logs), sorts them chronologically, and writes the `FINAL_TRANSCRIPT_PATH` file.

## üöÄ Setup & Installation

### System Dependencies (FFmpeg)
`pydub` requires **FFmpeg** to be installed on your system to handle `.flac` and `.wav` files.
* **Windows:** Download the binary from [ffmpeg.org](https://ffmpeg.org/download.html) and add it to your system's PATH.
* **macOS (via Homebrew):** `brew install ffmpeg`
* **Linux (via apt):** `sudo apt update && sudo apt install ffmpeg`

### Python Dependencies
It's highly recommended to use a virtual environment based on the `requirements.txt` file.
